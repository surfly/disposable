#!/usr/bin/env python3
from typing import Any, Union, Tuple
import functools
import json
import re
import sys
import hashlib
import html
import time
import tldextract
import argparse
import dns.resolver
import dns.rdatatype
import dns.exception
import concurrent.futures
import logging
import httpx
import ipaddress
import random
import string
from websocket import create_connection

def generate_random_string(length):
    letters = string.ascii_lowercase
    return ''.join(random.choice(letters) for _ in range(length))


class remoteData():
    retry_errors_re = re.compile(r"""(The read operation timed out|urlopen error timed out)""", re.I)

    @staticmethod
    def fetchFile(src: str, ignore_errors: bool = False) -> bytes:
        """Read local file

        :param src: filename
        :type src: str
        :param ignore_errors: ignore file errors / file does not exist
        :type ignore_errors: bool
        :return: data of entries
        :rtype: str
        """
        try:
            with open(src, 'rb') as f:
                return f.read()
        except FileNotFoundError as e:
            if ignore_errors:
                return b''
            raise e
        except IOError as e:
            if ignore_errors:
                return b''
            raise e

    @staticmethod
    def fetchWS(src: str) -> str:
        """Fetch data by websocket

        :param src: url to connect to
        :type src: str
        :return: raw data received by websocket
        :rtype: str
        """
        try:
            ws = create_connection(src)
            data = ''.join(ws.recv() + "\n" for _ in range(3))
            ws.close()
        except IOError as e:
            logging.exception(e)
            return ''
        return data

    @staticmethod
    def fetchHTTPRaw(url: str, headers: Union[dict, None] = None, timeout: int = 3, max_retry: int = 150) -> Union[httpx.Response, None]:
        """Fetch data from HTTP(s) URL and return http object

        :param url: URL to call
        :type url: str
        :param headers: additional headers, defaults to None
        :type headers: dict, optional
        :param timeout: timeout for call, defaults to 3
        :type timeout: int, optional
        :param max_retry: maximum number of retries, defaults to 150
        :type max_retry: int, optional
        :return: _description_
        :rtype: Union[http.client.HTTPResponse, None]
        """
        if headers is None:
            headers = {}
        retry = 0
        headers.setdefault('User-Agent', 'Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/116.0')
        with httpx.Client(http2=True, verify=False) as client:
            while retry < max_retry:
                try:
                    return client.get(url, headers=headers, timeout=timeout)
                except Exception as e:
                    retry += 1
                    logging.error(e)
                    if remoteData.retry_errors_re.search(str(e)) and retry < max_retry:
                        time.sleep(1)
                        continue

                    logging.warning('Fetching URL %s failed, see error: %s', url, e)
                    break
        return None

    @staticmethod
    def fetchHTTP(url: str, headers: Union[dict, None] = None, timeout: int = 3, max_retry: int = 150) -> bytes:
        """Fetch data from HTTP(s) URL and return content

        :param url: URL to call
        :type url: str
        :param headers: additional headers, defaults to None
        :type headers: dict, optional
        :param timeout: timeout for call, defaults to 3
        :type timeout: int, optional
        :param max_retry: maximum number of retries, defaults to 150
        :type max_retry: int, optional
        :return: read content as bytes
        :rtype: bytes
        """
        res = remoteData.fetchHTTPRaw(url, headers, timeout, max_retry)
        return (res and res.read()) or b''


class disposableHostGenerator():
    domain_regex = re.compile(r'^[a-z\d-]{1,63}(\.[a-z-\.]{2,63})+$')
    domain_search_regex = re.compile(r'["\'\s>]([a-z\d\.-]{1,63}\.[a-z\-]{2,63})["\'\s<]', re.I)
    html_generic_re = re.compile(r"""<option[^>]*>@?([a-z0-9\-\.\&#;\d+]+)\s*(\(PW\))?<\/option>""", re.I)
    sha1_regex = re.compile(r'^[a-fA-F0-9]{40}')

    sources = [
        {'type': 'list', 'src': 'https://gist.githubusercontent.com/adamloving/4401361/raw/'},
        {'type': 'list', 'src': 'https://gist.githubusercontent.com/jamesonev/7e188c35fd5ca754c970e3a1caf045ef/raw/'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/disposable/static-disposable-lists/master/mail-data-hosts-net.txt'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/wesbos/burner-email-providers/master/emails.txt'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/disposable/disposable/master/blacklist.txt'},
        {'type': 'list', 'src': 'https://www.stopforumspam.com/downloads/toxic_domains_whole.txt'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/martenson/disposable-email-domains/master/disposable_email_blocklist.conf'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/daisy1754/jp-disposable-emails/master/list.txt'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/FGRibreau/mailchecker/master/list.txt'},
        {'type': 'json', 'src': 'https://raw.githubusercontent.com/ivolo/disposable-email-domains/master/index.json'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/7c/fakefilter/main/txt/data.txt'},
        {'type': 'list', 'src': 'https://raw.githubusercontent.com/flotwig/disposable-email-addresses/master/domains.txt'},
        {'type': 'json', 'src': 'https://inboxes.com/api/v2/domain'},
        {'type': 'json', 'src': 'https://mob1.temp-mail.org/request/domains/format/json'},
        {'type': 'json', 'src': 'https://api.internal.temp-mail.io/api/v2/domains'},
        {'type': 'json', 'src': 'https://www.fakemail.net/index/index', 'scrape': True},
        {'type': 'json', 'src': 'https://api.mailpoof.com/domains'},
        {'type': 'file', 'src': 'blacklist.txt', 'ignore_not_exists': True},
        {'type': 'sha1', 'src': 'https://raw.githubusercontent.com/GeroldSetz/Mailinator-Domains/master/mailinator_domains_from_bdea.cc.txt'},
        {
            'type': 'html',
            'src': 'https://www.rotvpn.com/en/disposable-email',
            'regex': [
                re.compile(r"""<div class=\"container text-center\">\s+<div[^>]+>(.+?)</div>\s+</div>""", re.I | re.DOTALL),
                domain_search_regex
            ]
        },
        {'type': 'html', 'src': 'https://emailfake.com',
            'regex': re.compile(r"""change_dropdown_list[^"]+"[^>]+>@?([a-z0-9\.-]{1,128})""", re.I),
            'scrape': True},
        {'type': 'html', 'src': 'https://www.guerrillamail.com/en/'},
        {'type': 'html', 'src': 'https://www.trash-mail.com/inbox/'},
        {'type': 'html', 'src': 'https://mail-temp.com',
            'regex': re.compile(r"""change_dropdown_list[^"]+"[^>]+>@?([a-z0-9\.-]{1,128})""", re.I), 'scrape': True},
        # currently blocked by cloudflare - we probably need some kind of external service or undetected-chromedriver for this...
        # {'type': 'html', 'src': 'https://10minutemail.com/session/address', 'regex': re.compile(r""".+?@?([a-z0-9\.-]{1,128})""", re.I)},
        {'type': 'html', 'src': 'https://correotemporal.org', 'regex': domain_search_regex},
        {'type': 'html', 'src': 'https://fakemailgenerator.net',
            'regex': re.compile(r"""<a.+?data-mailhost=\"@?([a-z0-9\.-]{1,128})\"""", re.I)},
        {'type': 'html', 'src': 'https://nospam.today/home', 'regex': [
            re.compile(r"""wire:initial-data="(.+?domains[^\"]+)\""""),
            re.compile(r"""\&quot;domains\&quot;:\[([^\]]+)\]"""),
            re.compile(r"""\&quot;([^\&]+)\&quot;""")
        ]},
        {'type': 'html', 'src': 'https://www.luxusmail.org',
            'regex': re.compile(r"""<a.+?domain-selector\"[^>]+>@([a-z0-9\.-]{1,128})""", re.I)},
        {'type': 'html', 'src': 'https://www.temp-mails.com',
            'regex': re.compile(r"""<option.+?value="([^"]+)">\d+\s*\@""", re.I)},
        {'type': 'html', 'src': 'https://lortemail.dk'},
        {'type': 'html', 'src': 'https://tempmail.plus/en/',
            'regex': re.compile(r"""<button type=\"button\" class=\"dropdown-item\">([^<]+)</button>""", re.I)},
        {'type': 'html', 'src': 'https://spamok.nl/demo' + generate_random_string(8),
            'regex': re.compile(r"""<option\s+value="([^"]+)">""", re.I)},
        {'type': 'html', 'src': 'https://tempr.email',
            'regex': re.compile(r"""<option\s+value[^>]*>@?([a-z0-9\-\.\&#;\d+]+)\s*(\(PW\))?<\/option>""", re.I)},
        {'type': 'ws', 'src': 'wss://dropmail.me/websocket'},
        {'type': 'custom', 'src': 'Tempmailo', 'scrape': True}
    ]

    def __init__(self, options: Union[dict, None] = None, out_file: Union[str, None] = None):
        self.options = options or {}
        log_level = logging.INFO if self.options.get('verbose') else logging.WARN
        if self.options.get('debug'):
            log_level = logging.DEBUG
        logging.basicConfig(format="%(levelname)s: %(message)s", level=log_level)
        logger = logging.getLogger('tldextract')
        logger.setLevel('WARNING')

        self.no_mx = set()
        self.domains = set()
        self.sha1 = set()
        self.old_domains = set()
        self.old_sha1 = set()
        self.legacy_domains = set()
        self.source_map = {}
        self.skip = set()
        self.scrape = set()
        self.out_file = 'domains' if out_file is None else out_file

        if self.options.get('file'):
            self.sources.insert(0, {
                'type': 'file',
                'src': self.options['file']
            })

        # load remote URL if no custom list is defined
        if self.options.get('whitelist') is None:
            self.sources.insert(0, {
                'type': 'whitelist',
                'src': 'https://raw.githubusercontent.com/disposable/disposable/master/whitelist.txt'
            })
            self.options['whitelist'] = 'whitelist.txt'

        self.sources.insert(0, {
            'type': 'whitelist_file',
            'src': self.options.get('whitelist'),
            'ignore_not_exists': self.options.get('whitelist') == 'whitelist.txt'
        })

    def _fetchData(self, source: dict) -> Any:
        """Fetch remote data for given source

        :param source: source dict
        :type source: dict
        :return: Data of fetch method
        :rtype: Any
        """
        if source.get('type') in ('file', 'whitelist_file'):
            return remoteData.fetchFile(source.get('src'), source.get('ignore_not_exists'))
        elif source.get('type') == 'custom':
            return getattr(self, "_process%s" % source.get('src'))()
        elif source.get('type') == 'ws':
            return remoteData.fetchWS(source.get('src'))

        headers = {}
        if source.get('type') == 'json':
            headers['Accept'] = 'application/json, text/javascript, */*; q=0.01'
            headers['X-Requested-With'] = 'XMLHttpRequest'
        return remoteData.fetchHTTP(source.get('src'), headers, source.get('timeout', 3), self.options.get('max_retry'))

    def _preProcessData(self, source: dict, data: Union[list, bytes, str]) -> Union[list, bool]:
        """preProcess data return by fetch method

        :param source: source dict
        :type source: dict
        :param data: Raw data
        :type data: Union[list, bytes, str]
        :return: Parsed data or bool if failed/invalid
        :rtype: Union[list, bool]
        """
        if type(data) is list:
            return data

        fmt = source['type']
        if fmt == 'json':
            raw = {}
            try:
                raw = json.loads(data.decode(source.get('encoding', 'utf-8')))
            except Exception as e:
                if 'Unexpected UTF-8 BOM' in str(e):
                    raw = json.loads(data.decode('utf-8-sig'))

            if not raw:
                logging.warning('No data in json')
                return False

            if 'domains' in raw:
                raw = raw['domains']

            if 'email' in raw:
                s = re.search(r'^.+?@?([a-z0-9\.-]{1,128})$', raw['email'])
                if s:
                    raw = [s.group(1)]

            if not isinstance(raw, list):
                logging.warning('This URL does not contain a JSON array')
                return False
            return list(filter(lambda line: line and isinstance(line, str), raw))

        if fmt in ('whitelist', 'list', 'file', 'whitelist_file'):
            lines = []
            for line in data.splitlines():
                line = line.decode(source.get('encoding', 'utf-8')).strip()
                if line.startswith('#') or line == '':
                    continue
                lines.append(line)
            return lines

        if fmt == 'html':
            raw = data.decode(source.get('encoding', 'utf-8'))
            html_re = source.get('regex', self.html_generic_re)
            if type(html_re) is not list:
                html_re = [html_re, ]

            html_ipt = raw
            html_list = []
            for html_re_item in html_re:
                html_list = html_re_item.findall(html_ipt)
                html_ipt = '\n'.join(list(map(lambda o: o[0] if type(o) is tuple else o, html_list)))

            return list(map(lambda opt: html.unescape(opt[0]) if type(opt) is tuple else opt, html_list))

        if fmt == 'sha1':
            x = 0
            for sha1_str in [line.decode('ascii').lower() for line in data.splitlines()]:
                if not sha1_str or not self.sha1_regex.match(sha1_str):
                    continue

                x += 1
                self.sha1.add(sha1_str)
            if x < 1:
                logging.warning('SHA1 source did not return any valid sha1 hash')
            return True

        if fmt == 'ws':
            for line in data.splitlines():
                if line[0] == 'D':
                    return line[1:].split(',')

        return False

    def _postProcessData(self, source: dict, data: Union[bytes, str], lines: list) -> Union[bool, Tuple[int, int]]:
        """Post process data returned by preProcess method

        :param source: source dict
        :type source: dict
        :param data: raw data
        :type data: Union[bytes, str]
        :param lines: parsed lines
        :type lines: list
        :return: number of added domains
        :rtype: int
        """
        lines_filtered = [line.lower().strip(' .,;@') for line in lines]
        lines_filtered = list(filter(lambda line: self.checkValidDomain(line), lines_filtered))

        if not lines_filtered:
            lines_filtered = self.domain_search_regex.findall(str(data))

        if source['type'] in ('whitelist', 'whitelist_file'):
            for host in lines_filtered:
                self.skip.add(host)
            return True

        if not lines_filtered:
            logging.warning('No results for this source')
            return False

        self.source_map[source['src']] = self.scrape if source.get('scrape') else lines_filtered

        added_domains = 0
        added_scrape_domains = 0
        for host in lines_filtered:
            if host not in self.domains:
                self.domains.add(host)
                added_domains += 1

            self.legacy_domains.add(host)

            try:
                self.sha1.add(hashlib.sha1(host.encode('idna')).hexdigest())
            except Exception:
                pass

            if source.get('scrape') and host not in self.scrape:
                self.scrape.add(host)
                added_scrape_domains += 1

        if lines_filtered:
            logging.debug("Example domain: %s", lines_filtered[0])

        if source.get('scrape'):
            return added_scrape_domains, len(lines_filtered)

        return added_domains, len(lines_filtered)

    def process(self, source: dict) -> bool:
        """Fetch data of given source and process

        :param source: source dict
        :type source: dict
        :return: True if process was successfull
        :rtype: bool
        """
        logging.info("Process %s (%s)", source['src'], source['type'])

        max_scrape = 80
        scrape_max_retry = 3
        scrape_count = 0
        self.scrape = set()
        scrape_retry = 0

        while scrape_count < max_scrape:
            data = self._fetchData(source)
            if data is None:
                logging.warning("No results by %s", source['src'])
                return False

            logging.debug("Fetched %s bytes", len(data))
            lines = self._preProcessData(source, data)
            if type(lines) is bool:
                return lines

            res = self._postProcessData(source, data, lines)
            if type(res) is bool:
                return res

            (processed_entries, found_entries) = res

            logging.debug('Processed %s entries (%s found)', processed_entries, found_entries)
            if source.get('scrape'):
                if processed_entries:
                    scrape_retry = 0
                else:
                    scrape_retry += 1
                    if scrape_retry > scrape_max_retry:
                        return True
                scrape_count += 1
                time.sleep(source.get('timeout', 8))
                continue
            return True
        return False

    def _processTempmailo(self) -> Union[None, list]:
        """Process source for tempmailo.com

        :return: Either list domains or None if failed
        :rtype: Union[None, list]
        """
        res = remoteData.fetchHTTPRaw('https://tempmailo.com/')
        if res is None:
            return None

        cookies = {}
        for (ky, vl) in res.getheaders():
            if ky.lower() != 'set-cookie':
                continue

            (ck_name, ck_data) = vl.split('=', 1)
            if ck_name.startswith('__'):
                continue
            (ck_value, _) = ck_data.split(';', 1)
            cookies[ck_name] = ck_value

        body = res.read().decode('utf8')

        f = re.search('name="__RequestVerificationToken".+?value="([^"]+)"', body)
        if not f:
            logging.warning('Failed to fetch __RequestVerificationToken')
            return None

        headers = {
            'requestverificationtoken': f.group(1),
            'accept': 'application/json, text/plain, */*',
            'x-requested-with': 'XMLHttpRequest',
            'referer': 'https://tempmailo.com/',
            'cookie': '; '.join(['%s=%s' % (ky, vl) for ky, vl in cookies.items()])
        }

        data = remoteData.fetchHTTP('https://tempmailo.com/changemail', headers=headers)
        if not data:
            logging.warning('Failed to fetch changemail endpoint')
            return None

        lines = []
        for line in data.splitlines():
            (_, domain) = line.decode('utf8').split('@', 1)
            lines.append(domain)

        return lines

    def readFiles(self):
        """ read and compare to current (old) domains file
        """
        self.old_domains = set()
        try:
            with open(self.out_file + '.txt') as f:
                for line in f:
                    self.old_domains.add(line.strip())
        except IOError:
            pass

        self.old_sha1 = set()
        try:
            with open(self.out_file + '_sha1.txt') as f:
                for line in f:
                    self.old_sha1.add(line.strip())
        except IOError:
            pass

        self.legacy_domains = set()
        try:
            with open(self.out_file + '_legacy.txt') as f:
                for line in f:
                    self.legacy_domains.add(line.strip())
        except IOError:
            pass

    def checkValidDomain(self, host: str) -> bool:
        """check if given host is not a TLD and a valid domainname

        :param host: host to validate
        :type host: str
        :return: true if valid domain
        :rtype: bool
        """
        try:
            if not self.domain_regex.match(host):
                return False

            t = tldextract.extract(host)
            return (t.domain != '' and t.suffix != '')
        except Exception:
            pass

        return False

    @staticmethod
    @functools.lru_cache(maxsize=1024*1024)
    def resolveDNS(resolver: dns.resolver.Resolver, host: str, type_id: int) -> Union[None, str, dns.resolver.Answer]:
        """Resolve given hostname against given resolver and return error or result

        :param resolver: Resolver to use
        :type resolver: dns.resolver.Resolver
        :param host: hostname to resolve
        :type host: str
        :param type_id: Type of request
        :type type_id: int
        :return: Str or None on error, or dns answer object
        :rtype: Union[None, dns.resolver.Answer, str]
        """
        r = None
        try:
            r = resolver.query(host, type_id)
        except KeyboardInterrupt:
            raise
        except dns.resolver.NXDOMAIN:
            return 'resolved but no entry'
        except dns.resolver.NoNameservers:
            return 'answer refused'
        except dns.resolver.NoAnswer:
            return 'no answer section'
        except dns.exception.Timeout:
            return 'timeout'
        except Exception as e:
            pass

        return r

    @staticmethod
    def fetchMX(domain: str, nameservers: list = None, dnsport: int = 53, resolver_timeout: int = 20) -> tuple:
        """Check if given domain has a valid MX entry

        :param domain: domain name to validate
        :type domain: str
        :param nameservers: list of nameservers to use for resolve
        :type nameservers: list
        :param dnsport: set port of dns server
        :type dnsport: int
        :return: tuple (domain name, bool valid)
        :rtype: tuple
        """
        if nameservers is None:
            nameservers = []
        resolver = dns.resolver.Resolver()
        resolver.lifetime = resolver.timeout = resolver_timeout

        if nameservers:
            resolver.nameservers = nameservers

        if dnsport:
            resolver.port = dnsport

        rq = [(domain, dns.rdatatype.MX, 'MX'), ]

        while rq:
            resolve = rq.pop()
            r = disposableHostGenerator.resolveDNS(resolver, resolve[0], resolve[1])

            if type(r) is str:
                logging.debug("%20s: resolved %20s (%2s): %s", domain, resolve[0], resolve[2], r)
                r = None

            if resolve[1] == dns.rdatatype.MX:
                mx_list = []
                if r:
                    mx_list = {rr.exchange.to_text(rr.exchange).lower() for rr in r.rrset}

                logging.debug("%20s: resolved %20s (%2s): %s", domain, resolve[0], resolve[2], mx_list)
                if mx_list:
                    if ('.' in mx_list or mx_list == ['localhost']):
                        return (domain, False)

                    rq.extend((x, dns.rdatatype.A, 'A') for x in mx_list)
                else:
                    rq.append((domain, dns.rdatatype.A, 'A'))
            elif r:
                invalid_ip = False
                ips = []
                for _r in r:
                    _ipr = None
                    try:
                        ips.append(_r.address)
                        _ipr = ipaddress.ip_address(_r.address)
                    except Exception:
                        invalid_ip = True
                        break

                    if not _ipr or _ipr.is_private or _ipr.is_reserved or _ipr.is_loopback or _ipr.is_multicast:
                        invalid_ip = True
                        break

                logging.debug("%20s: resolved %20s (%2s): %s (invalid: %s)", domain, resolve[0], resolve[2], ips, invalid_ip)

                if not invalid_ip:
                    return (domain, True)

        return (domain, False)

    def listSources(self):
        """list all available sources
        """
        for source in self.sources:
            logging.info("Source %12s: %s", source.get('type'), source.get('src'))

    def generate(self):
        """Fetch all data + generate lists
        """
        # fetch data from sources
        for source in self.sources:
            if source['src'] not in 'whitelist_file' and \
               self.options.get('src_filter') is not None and \
               source['src'] != self.options.get('src_filter'):
                continue

            try:
                if not self.process(source) and self.options.get('debug'):
                    raise RuntimeError("No result for %s" % source)
            except Exception as err:
                logging.exception(err)
                raise err

        # remove all domains listed in whitelist from result set
        for domain in self.skip:
            try:
                self.domains.remove(domain)
            except KeyError:
                pass
            try:
                self.sha1.remove(hashlib.sha1(domain.encode('idna')).hexdigest())
            except KeyError:
                pass

            if self.options.get('dns_verify') and domain not in ('example.com', 'example.org', 'example.net'):
                r = disposableHostGenerator.fetchMX(domain, self.options.get(
                    'nameservers'), self.options.get('dnsport'), self.options.get('dns_timeout', 20))
                if not r or not r[1]:
                    logging.warning('Whitelist domain %s does not resolve!', domain)

        # MX verify check
        self.no_mx = []
        if self.options.get('dns_verify'):
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.options.get('dns_threads', 1)) as executor:
                futures = [executor.submit(disposableHostGenerator.fetchMX, domain,
                                           self.options.get('nameservers'), self.options.get('dnsport'), self.options.get('dns_timeout', 20))
                           for domain in self.domains]
                for future in concurrent.futures.as_completed(futures):
                    (domain, valid) = future.result()
                    if not valid:
                        self.no_mx.append(domain)

        if self.options.get('verbose'):
            if not self.old_domains:
                self.readFiles()

            added = list(
                filter(lambda domain: domain not in self.old_domains, self.domains))
            removed = list(
                filter(lambda domain: domain not in self.domains, self.old_domains))

            added_sha1 = list(
                filter(lambda sha_str: sha_str not in self.old_sha1, self.sha1))
            removed_sha1 = list(
                filter(lambda sha_str: sha_str not in self.sha1, self.old_sha1))

            logging.info('Fetched %s domains and %s hashes', len(self.domains), len(self.sha1))
            if self.options.get('dns_verify'):
                logging.info(' - %s domain(s) have no MX', len(self.no_mx))
                if self.options.get('list_no_mx'):
                    logging.info('No MX: %s', self.no_mx)
            logging.info(' - %s domain(s) added', len(added))
            logging.info(' - %s domain(s) removed', len(removed))
            logging.info(' - %s hash(es) added', len(added_sha1))
            logging.info(' - %s hash(es) removed', len(removed_sha1))
            # stop if nothing has changed
            if len(added) == len(removed) == len(added_sha1) == len(removed_sha1) == 0:
                return False

            if self.options.get('src_filter'):
                logging.info("Fetched: %s", self.domains)

        return True

    def writeToFile(self):
        """write new list to file(s)
        """
        domains = sorted(self.domains)
        with open(self.out_file + '.txt', 'w') as ff:
            ff.write('\n'.join(domains))

        with open(self.out_file + '.json', 'w') as ff:
            ff.write(json.dumps(domains))

        if self.options.get('source_map'):
            with open(self.out_file + '_source_map.txt', 'w') as ff:
                for (src_url, source_map_domains) in sorted(self.source_map.items()):
                    ff.write(src_url + ':' + ('\n%s:' % src_url).join(sorted(source_map_domains)) + "\n")

        if self.no_mx:
            domains_with_mx = self.domains
            for domain in self.no_mx:
                try:
                    domains_with_mx.remove(domain)
                except KeyError:
                    pass

            domains = sorted(domains_with_mx)
            with open(self.out_file + '_mx.txt', 'w') as ff:
                ff.write('\n'.join(domains))

            with open(self.out_file + '_mx.json', 'w') as ff:
                ff.write(json.dumps(domains))

        # write new hash list to file(s)
        domains_sha1 = sorted(self.sha1)
        with open(self.out_file + '_sha1.txt', 'w') as ff:
            ff.write('\n'.join(domains_sha1))

        with open(self.out_file + '_sha1.json', 'w') as ff:
            ff.write(json.dumps(domains_sha1))


if __name__ == '__main__':
    exit_status = 1
    parser = argparse.ArgumentParser(description='Generate list of dispsable mail hosts.')
    parser.add_argument('--dns-verify', action='store_true', dest='dns_verify',
                        help='validate if valid MX / A record is present for hosts')
    parser.add_argument('--source-map', action='store_true', dest='source_map', help='generate source map')
    parser.add_argument('--src', dest='src_filter', help='only request entries for given source')
    parser.add_argument('-q', '--quiet', action='store_false', dest='verbose', help='hide verbose output')
    parser.add_argument('-D', '--debug', action='store_true', dest='debug', help='show debug output and exit on warn/error')
    parser.add_argument('--max-retry', type=int, dest='max_retry',
                        help='maximum count of retries to fetch an url, default: 150', default=150)
    parser.add_argument('--dns-threads', type=int, dest='dns_threads',
                        help='count of threads to use for dns resolving, default:10', default=10)
    parser.add_argument('--dns-timeout', type=int, dest="dns_timeout", help='timeout for dns request, default: 20', default=20.0)
    parser.add_argument('--ns', action='append', dest='nameservers', help='set custom resolver for dns-verify')
    parser.add_argument('--dnsport', type=int, dest='dnsport', help='set custom resolver port for dns-verify')
    parser.add_argument('--list-sources', action='store_true', dest='list_sources', help='list all sources')
    parser.add_argument('--list-no-mx', action='store_true', dest='list_no_mx', help='list domains without valid mx')
    parser.add_argument('--whitelist', dest='whitelist',
                        help='custom whitelist to load - all domains listed in that file are ignored in blacklist')
    parser.add_argument('--file', dest='file', help='custom file to load - add custom domains to local result')

    options = parser.parse_args()
    dhg = disposableHostGenerator(vars(options))
    if options.list_sources:
        dhg.listSources()
    elif dhg.generate() or options.src_filter is not None:
        exit_status = 0
        dhg.writeToFile()
    sys.exit(exit_status)
